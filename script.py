# -*- coding: utf-8 -*-
"""script.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gS_FaR_6a1ycMb3IR4q9vhG5cLYgPAAz
"""

#!pip install --upgrade transformers datasets scikit-learn

import pandas as pd
import torch
import transformers
from packaging import version
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import f1_score
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments

print(f"Transformers version found: {transformers.__version__}")

use_modern_args = version.parse(transformers.__version__) >= version.parse("4.0.0")

if use_modern_args:
    print("--> Modern 'transformers' version detected. Using 'evaluation_strategy'.")
else:
    print("--> Older 'transformers' version detected. Using 'evaluate_during_training'.")

MODEL_NAME = 'roberta-base'
TRAIN_FILE_PATH = 'trainset.json'
MAX_LENGTH = 512
TARGET_FOLD = 0

raw_data = pd.read_json(TRAIN_FILE_PATH)

rows = []
for _, row in raw_data.iterrows():
    for tutor, details in row['tutor_responses'].items():
        rows.append({
            'history': row['conversation_history'],
            'response': details['response'],
            'mistake_label': details.get('annotation', {}).get('Mistake_Identification')
        })
df = pd.DataFrame(rows).dropna(subset=['mistake_label'])


label_map = {'No': 0, 'To some extent': 1, 'Yes': 2}
df['mistake_label_num'] = df['mistake_label'].map(label_map)

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
df['fold'] = -1
for fold, (train_idx, val_idx) in enumerate(skf.split(df, df['mistake_label_num'])):
    df.loc[val_idx, 'fold'] = fold

class DatathonDataset(torch.utils.data.Dataset):
    def __init__(self, df, tokenizer, max_length):
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.texts = (df['history'] + " [SEP] " + df['response']).tolist()
        self.labels = df['mistake_label_num'].tolist()
    def __len__(self):
        return len(self.texts)
    def __getitem__(self, idx):
        encoding = self.tokenizer(self.texts[idx], truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')
        item = {key: val.squeeze() for key, val in encoding.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item


tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
train_df = df[df['fold'] != TARGET_FOLD]
val_df = df[df['fold'] == TARGET_FOLD]
train_dataset = DatathonDataset(train_df, tokenizer, MAX_LENGTH)
val_dataset = DatathonDataset(val_df, tokenizer, MAX_LENGTH)
model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(label_map))

def compute_metrics(p):
    preds = p.predictions.argmax(-1)
    labels = p.label_ids
    return {'macro_f1': f1_score(labels, preds, average='macro')}



training_args = TrainingArguments(
    output_dir=f'./results_fold_{TARGET_FOLD}',
    num_train_epochs=3,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    learning_rate=1e-5,
    weight_decay=0.01,
    load_best_model_at_end=False,
    metric_for_best_model='macro_f1',
    greater_is_better=True,
    report_to='none',

)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics,
)


print("--> Starting training...")
trainer.train()
print("--> Training complete!")

trainer.evaluate()

# ======================================================================================
# === Step 1: Setup and Environment
# ======================================================================================
!pip install --upgrade transformers datasets scikit-learn

import pandas as pd
import torch
import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import f1_score
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments


# ======================================================================================
# === Step 2: Configuration and Data Loading
# ======================================================================================
# --- Use the more powerful model and tuned hyperparameters ---
MODEL_NAME = 'roberta-base'
TRAIN_FILE_PATH = 'trainset.json'
DEV_TEST_FILE_PATH = 'dev_testset.json'
MAX_LENGTH = 512
NUM_FOLDS = 5

# --- Load and Flatten Training Data ---
raw_train_data = pd.read_json(TRAIN_FILE_PATH)
rows = []
for _, row in raw_train_data.iterrows():
    for tutor, details in row['tutor_responses'].items():
        rows.append({
            'history': row['conversation_history'],
            'response': details['response'],
            'mistake_label': details.get('annotation', {}).get('Mistake_Identification')
        })
df = pd.DataFrame(rows).dropna(subset=['mistake_label'])

# --- Load and Flatten Test Data ---
raw_test_data = pd.read_json(DEV_TEST_FILE_PATH)
test_rows = []
for _, row in raw_test_data.iterrows():
    test_rows.append({
        'conversation_id': row['conversation_id'],
        'tutor': list(row['tutor_responses'].keys())[0], # Assumes one response per entry in dev-test
        'history': row['conversation_history'],
        'response': list(row['tutor_responses'].values())[0]['response']
    })
test_df = pd.DataFrame(test_rows)

# ======================================================================================
# === Step 3: Folds and Dataset Classes
# ======================================================================================
label_map = {'No': 0, 'To some extent': 1, 'Yes': 2}
df['mistake_label_num'] = df['mistake_label'].map(label_map)

skf = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=42)
df['fold'] = -1
for fold, (train_idx, val_idx) in enumerate(skf.split(df, df['mistake_label_num'])):
    df.loc[val_idx, 'fold'] = fold

class DatathonDataset(torch.utils.data.Dataset):
    def __init__(self, df, tokenizer, max_length):
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.texts = (df['history'] + " [SEP] " + df['response']).tolist()
        self.labels = df['mistake_label_num'].tolist()
    def __len__(self): return len(self.texts)
    def __getitem__(self, idx):
        encoding = self.tokenizer(self.texts[idx], truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')
        item = {key: val.squeeze() for key, val in encoding.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

class TestDataset(torch.utils.data.Dataset):
    def __init__(self, df, tokenizer, max_length):
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.texts = (df['history'] + " [SEP] " + df['response']).tolist()
    def __len__(self): return len(self.texts)
    def __getitem__(self, idx):
        encoding = self.tokenizer(self.texts[idx], truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')
        return {key: val.squeeze() for key, val in encoding.items()}

# ======================================================================================
# === Step 4: Cross-Validation Loop
# ======================================================================================
print(f"--> Starting {NUM_FOLDS}-fold cross-validation with model: {MODEL_NAME}")

all_val_scores = []
all_test_predictions = []
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
test_dataset = TestDataset(test_df, tokenizer, MAX_LENGTH)

for fold in range(NUM_FOLDS):
    print(f"\n===== FOLD {fold} =====")

    # 1. Split data
    train_df = df[df['fold'] != fold]
    val_df = df[df['fold'] == fold]
    train_dataset = DatathonDataset(train_df, tokenizer, MAX_LENGTH)
    val_dataset = DatathonDataset(val_df, tokenizer, MAX_LENGTH)

    # 2. Initialize a new model for each fold to avoid weight leakage
    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(label_map))

    # 3. Define TrainingArguments
    training_args = TrainingArguments(
        output_dir=f'./results_fold_{fold}',
        num_train_epochs=3,
        learning_rate=1e-5,
        per_device_train_batch_size=8, # Reduce to 4 if you get memory errors
        per_device_eval_batch_size=8,
        weight_decay=0.01,
        evaluation_strategy="epoch", # Corrected parameter
        save_strategy="epoch", # Added parameter
        load_best_model_at_end=True,
        metric_for_best_model='macro_f1',
        greater_is_better=True,
        report_to='none',
    )

    # 4. Initialize and train
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        compute_metrics=lambda p: {'macro_f1': f1_score(p.label_ids, p.predictions.argmax(-1), average='macro')}
    )
    trainer.train()

    # 5. Evaluate and store score
    eval_results = trainer.evaluate()
    fold_score = eval_results['eval_macro_f1']
    all_val_scores.append(fold_score)
    print(f"--> Fold {fold} Macro F1 Score: {fold_score:.4f}")

    # 6. Store test predictions (logits)
    test_predictions = trainer.predict(test_dataset)
    all_test_predictions.append(test_predictions.predictions)

# ======================================================================================
# === Step 5: Final Score and Submission File
# ======================================================================================
# --- Calculate and print the final CV score ---
mean_score = np.mean(all_val_scores)
std_dev = np.std(all_val_scores)
print(f"\n--> Final CV Macro F1 Score: {mean_score:.4f} +/- {std_dev:.4f}")

# --- Average predictions and create submission file ---
# Average the logits from all 5 models
avg_preds = np.mean(all_test_predictions, axis=0)
# Get the final predicted class by finding the max logit
final_preds_num = np.argmax(avg_preds, axis=1)

reverse_label_map = {0: 'No', 1: 'To some extent', 2: 'Yes'}
final_preds_text = [reverse_label_map[i] for i in final_preds_num]

submission_df = test_df[['conversation_id', 'tutor']].copy()
submission_df['prediction'] = final_preds_text
submission_df.to_csv('submission_cv.csv', index=False)

print("\n--> 'submission_cv.csv' created successfully from averaged predictions!")
print(submission_df.head())